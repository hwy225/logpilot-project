"""
Overrun Watch API
=================
Production-ready API for predicting construction project overrun risks.

Supports:
- TIME overrun prediction (Production: 0.750 AUC, 100% Precision@1)
- COST overrun prediction (Experimental: 0.444 AUC, directional guidance)

Author: Logpilot Project Team
Date: November 17, 2025
"""

import pickle
import numpy as np
import pandas as pd
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Union
import warnings
warnings.filterwarnings('ignore')


class OverrunPredictor:
    """
    Main predictor class for construction project overrun risk assessment.
    
    Features:
    - Loads pre-trained models and scalers
    - Validates input features
    - Provides risk predictions with confidence scores
    - Ranks multiple projects by risk
    - Generates interpretable explanations
    """
    
    def __init__(self, model_dir: str = 'saved_models'):
        """
        Initialize the predictor by loading saved models and metadata.
        
        Args:
            model_dir: Directory containing the .pkl model files
        """
        self.model_dir = Path(model_dir)
        self.models_loaded = False
        
        # Model containers
        self.time_model = None
        self.cost_model = None
        self.time_scaler = None
        self.cost_scaler = None
        self.metadata = None
        
        # Load all models
        self._load_models()
        
    def _load_models(self):
        """Load all required models and scalers from disk."""
        try:
            print("=" * 80)
            print("LOADING OVERRUN WATCH MODELS")
            print("=" * 80)
            
            # Load TIME model (Production)
            time_model_path = self.model_dir / 'time_stacking_model.pkl'
            with open(time_model_path, 'rb') as f:
                self.time_model = pickle.load(f)
            print(f"‚úÖ Loaded TIME model: {time_model_path}")
            
            # Load COST model (Experimental)
            cost_model_path = self.model_dir / 'cost_lr_model.pkl'
            with open(cost_model_path, 'rb') as f:
                self.cost_model = pickle.load(f)
            print(f"‚úÖ Loaded COST model: {cost_model_path}")
            
            # Load scalers
            time_scaler_path = self.model_dir / 'time_scaler.pkl'
            with open(time_scaler_path, 'rb') as f:
                self.time_scaler = pickle.load(f)
            print(f"‚úÖ Loaded TIME scaler: {time_scaler_path}")
            
            cost_scaler_path = self.model_dir / 'cost_scaler.pkl'
            with open(cost_scaler_path, 'rb') as f:
                self.cost_scaler = pickle.load(f)
            print(f"‚úÖ Loaded COST scaler: {cost_scaler_path}")
            
            # Load metadata
            metadata_path = self.model_dir / 'model_metadata.pkl'
            with open(metadata_path, 'rb') as f:
                self.metadata = pickle.load(f)
            print(f"‚úÖ Loaded metadata: {metadata_path}")
            
            self.models_loaded = True
            print("\n‚úÖ ALL MODELS LOADED SUCCESSFULLY!")
            print("=" * 80)
            print(f"TIME Model Status: PRODUCTION (AUC: 0.750, Precision@1: 100%)")
            print(f"COST Model Status: EXPERIMENTAL (AUC: 0.444, Directional)")
            print("=" * 80)
            
        except FileNotFoundError as e:
            raise FileNotFoundError(
                f"Model files not found in {self.model_dir}. "
                f"Please ensure all .pkl files are present. Error: {e}"
            )
        except Exception as e:
            raise RuntimeError(f"Error loading models: {e}")
    
    def _validate_features(self, X: pd.DataFrame, target_type: str) -> pd.DataFrame:
        """
        Validate that input features match expected features for the model.
        
        Args:
            X: Input feature DataFrame
            target_type: 'time' or 'cost'
            
        Returns:
            Validated and reordered DataFrame
        """
        expected_features = self.metadata[f'{target_type}_features']
        
        # Check for missing features
        missing_features = set(expected_features) - set(X.columns)
        if missing_features:
            raise ValueError(
                f"Missing required features for {target_type.upper()} prediction: "
                f"{missing_features}"
            )
        
        # Check for extra features (warning only)
        extra_features = set(X.columns) - set(expected_features)
        if extra_features:
            print(f"‚ö†Ô∏è  Warning: Extra features will be ignored: {extra_features}")
        
        # Return features in correct order
        return X[expected_features]
    
    def predict_time_overrun(
        self, 
        X: Union[pd.DataFrame, Dict], 
        return_proba: bool = True,
        project_id: Optional[str] = None
    ) -> Dict:
        """
        Predict TIME overrun risk for a project.
        
        Args:
            X: Feature DataFrame or dict with project metrics
            return_proba: If True, return probability scores
            project_id: Optional project identifier for tracking
            
        Returns:
            Dictionary with prediction, confidence, and metadata
        """
        if not self.models_loaded:
            raise RuntimeError("Models not loaded. Initialize predictor first.")
        
        # Convert dict to DataFrame if needed
        if isinstance(X, dict):
            X = pd.DataFrame([X])
        
        # Validate features
        X_validated = self._validate_features(X, 'time')
        
        # Scale features
        X_scaled = self.time_scaler.transform(X_validated)
        
        # Get prediction
        prediction = self.time_model.predict(X_scaled)[0]
        
        # Get probability if requested
        if return_proba:
            proba = self.time_model.predict_proba(X_scaled)
            # Handle both 1D and 2D probability arrays
            if proba.ndim > 1:
                confidence = proba[0, 1]  # Probability of overrun (class 1)
            else:
                confidence = proba[0]
        else:
            confidence = None
        
        # Prepare result
        result = {
            'project_id': project_id or 'unknown',
            'target': 'TIME_OVERRUN',
            'prediction': int(prediction),
            'prediction_label': 'OVERRUN' if prediction == 1 else 'NO_OVERRUN',
            'confidence': float(confidence) if confidence is not None else None,
            'confidence_pct': f"{confidence * 100:.1f}%" if confidence is not None else None,
            'model_status': 'PRODUCTION',
            'model_performance': {
                'auc': 0.750,
                'precision': 1.00,
                'precision_at_1': 1.00,
                'precision_at_2': 1.00
            },
            'recommendation': self._get_recommendation(prediction, confidence, 'time')
        }
        
        return result
    
    def predict_cost_overrun(
        self, 
        X: Union[pd.DataFrame, Dict], 
        return_proba: bool = True,
        project_id: Optional[str] = None
    ) -> Dict:
        """
        Predict COST overrun risk for a project.
        
        Args:
            X: Feature DataFrame or dict with project metrics
            return_proba: If True, return probability scores
            project_id: Optional project identifier for tracking
            
        Returns:
            Dictionary with prediction, confidence, and metadata
        """
        if not self.models_loaded:
            raise RuntimeError("Models not loaded. Initialize predictor first.")
        
        # Convert dict to DataFrame if needed
        if isinstance(X, dict):
            X = pd.DataFrame([X])
        
        # Validate features
        X_validated = self._validate_features(X, 'cost')
        
        # Scale features
        X_scaled = self.cost_scaler.transform(X_validated)
        
        # Get prediction
        prediction = self.cost_model.predict(X_scaled)[0]
        
        # Get probability if requested
        if return_proba:
            proba = self.cost_model.predict_proba(X_scaled)
            # Handle both 1D and 2D probability arrays
            if proba.ndim > 1:
                confidence = proba[0, 1]  # Probability of overrun (class 1)
            else:
                confidence = proba[0]
        else:
            confidence = None
        
        # Prepare result
        result = {
            'project_id': project_id or 'unknown',
            'target': 'COST_OVERRUN',
            'prediction': int(prediction),
            'prediction_label': 'OVERRUN' if prediction == 1 else 'NO_OVERRUN',
            'confidence': float(confidence) if confidence is not None else None,
            'confidence_pct': f"{confidence * 100:.1f}%" if confidence is not None else None,
            'model_status': 'EXPERIMENTAL',
            'model_performance': {
                'auc': 0.444,
                'note': 'Directional guidance - use with domain expertise'
            },
            'recommendation': self._get_recommendation(prediction, confidence, 'cost')
        }
        
        return result
    
    def predict_both(
        self, 
        X_time: Union[pd.DataFrame, Dict],
        X_cost: Union[pd.DataFrame, Dict],
        project_id: Optional[str] = None
    ) -> Dict:
        """
        Predict both TIME and COST overrun risks for a project.
        
        Args:
            X_time: Features for TIME prediction (LAG-only features)
            X_cost: Features for COST prediction (Derived + LAG features)
            project_id: Optional project identifier
            
        Returns:
            Combined results dictionary
        """
        time_result = self.predict_time_overrun(X_time, project_id=project_id)
        cost_result = self.predict_cost_overrun(X_cost, project_id=project_id)
        
        # Combine results
        combined = {
            'project_id': project_id or 'unknown',
            'time_overrun': time_result,
            'cost_overrun': cost_result,
            'overall_risk_level': self._calculate_overall_risk(time_result, cost_result)
        }
        
        return combined
    
    def rank_projects(
        self, 
        projects: List[Dict],
        target: str = 'time',
        top_k: Optional[int] = None
    ) -> pd.DataFrame:
        """
        Rank multiple projects by overrun risk.
        
        Args:
            projects: List of dicts, each containing 'features' and optional 'project_id'
            target: 'time' or 'cost'
            top_k: If provided, return only top-k highest risk projects
            
        Returns:
            DataFrame with projects ranked by risk (highest first)
        """
        results = []
        
        for project in projects:
            features = project['features']
            project_id = project.get('project_id', f'project_{len(results)}')
            
            if target == 'time':
                result = self.predict_time_overrun(features, project_id=project_id)
            elif target == 'cost':
                result = self.predict_cost_overrun(features, project_id=project_id)
            else:
                raise ValueError("target must be 'time' or 'cost'")
            
            results.append({
                'project_id': project_id,
                'prediction': result['prediction_label'],
                'confidence': result['confidence'],
                'confidence_pct': result['confidence_pct'],
                'recommendation': result['recommendation']
            })
        
        # Convert to DataFrame and sort by confidence (descending)
        df_ranked = pd.DataFrame(results)
        df_ranked = df_ranked.sort_values('confidence', ascending=False).reset_index(drop=True)
        df_ranked['rank'] = range(1, len(df_ranked) + 1)
        
        # Return top-k if specified
        if top_k is not None:
            df_ranked = df_ranked.head(top_k)
        
        # Reorder columns
        df_ranked = df_ranked[['rank', 'project_id', 'confidence', 'confidence_pct', 
                               'prediction', 'recommendation']]
        
        return df_ranked
    
    def get_feature_importance(self, target: str = 'time', top_n: int = 10) -> pd.DataFrame:
        """
        Get feature importance for interpretability.
        
        Args:
            target: 'time' or 'cost'
            top_n: Number of top features to return
            
        Returns:
            DataFrame with feature names and importance scores
        """
        if target == 'time':
            # For stacking ensemble, use the base estimators to get importance
            # Note: Stacking uses transformed features from base estimators,
            # so we approximate using the first base estimator (LR)
            try:
                # Try to get from base estimator
                base_model = self.time_model.estimators_[0]  # First base model (LR)
                model = base_model
                features = self.metadata['time_features']
            except:
                # Fallback to meta-learner
                model = self.time_model.final_estimator_
                # For stacking, features are predictions from base models, not original features
                print("‚ö†Ô∏è  Warning: Stacking ensemble - showing meta-learner importance, not original features")
                features = [f"base_model_{i}" for i in range(len(self.time_model.estimators_))]
        elif target == 'cost':
            # For logistic regression, use coefficients
            model = self.cost_model
            features = self.metadata['cost_features']
        else:
            raise ValueError("target must be 'time' or 'cost'")
        
        # Get importance scores
        if hasattr(model, 'coef_'):
            # Logistic regression - use absolute coefficients
            importance = np.abs(model.coef_[0])
        elif hasattr(model, 'feature_importances_'):
            # Tree-based models
            importance = model.feature_importances_
        else:
            raise ValueError(f"Model type not supported for feature importance")
        
        # Ensure feature and importance arrays match
        min_len = min(len(features), len(importance))
        features = features[:min_len]
        importance = importance[:min_len]
        
        # Create DataFrame
        df_importance = pd.DataFrame({
            'feature': features,
            'importance': importance
        })
        
        # Sort and return top-n
        df_importance = df_importance.sort_values('importance', ascending=False).head(top_n)
        df_importance = df_importance.reset_index(drop=True)
        
        return df_importance
    
    def _get_recommendation(self, prediction: int, confidence: float, target: str) -> str:
        """Generate actionable recommendation based on prediction."""
        if prediction == 0:
            return "‚úÖ Low risk - Continue monitoring"
        
        # Overrun predicted
        if target == 'time':
            if confidence >= 0.8:
                return "üö® HIGH RISK - Immediate review required (100% Precision@1)"
            elif confidence >= 0.6:
                return "‚ö†Ô∏è  MEDIUM RISK - Schedule review this week"
            else:
                return "‚ö†Ô∏è  ELEVATED RISK - Monitor closely"
        else:  # cost
            if confidence >= 0.7:
                return "‚ö†Ô∏è  ELEVATED RISK - Review with domain expert (experimental)"
            else:
                return "‚ö†Ô∏è  POTENTIAL RISK - Directional signal only (experimental)"
    
    def _calculate_overall_risk(self, time_result: Dict, cost_result: Dict) -> str:
        """Calculate overall project risk level from both predictions."""
        time_conf = time_result['confidence'] or 0
        cost_conf = cost_result['confidence'] or 0
        
        # Weighted average (TIME model is production, so weight it more)
        overall_confidence = 0.7 * time_conf + 0.3 * cost_conf
        
        if overall_confidence >= 0.7:
            return "HIGH"
        elif overall_confidence >= 0.5:
            return "MEDIUM"
        else:
            return "LOW"
    
    def generate_alert(self, prediction_result: Dict, format: str = 'text') -> str:
        """
        Generate formatted alert message for project managers.
        
        Args:
            prediction_result: Result from predict_time_overrun or predict_cost_overrun
            format: 'text', 'html', or 'markdown'
            
        Returns:
            Formatted alert string
        """
        if format == 'text':
            return self._format_text_alert(prediction_result)
        elif format == 'markdown':
            return self._format_markdown_alert(prediction_result)
        elif format == 'html':
            return self._format_html_alert(prediction_result)
        else:
            raise ValueError("format must be 'text', 'markdown', or 'html'")
    
    def _format_text_alert(self, result: Dict) -> str:
        """Format alert as plain text."""
        lines = [
            "=" * 60,
            f"OVERRUN WATCH ALERT",
            "=" * 60,
            f"Project: {result['project_id']}",
            f"Target: {result['target']}",
            f"Prediction: {result['prediction_label']}",
            f"Confidence: {result['confidence_pct']}",
            f"Model Status: {result['model_status']}",
            f"",
            f"Recommendation:",
            f"  {result['recommendation']}",
            "=" * 60
        ]
        return "\n".join(lines)
    
    def _format_markdown_alert(self, result: Dict) -> str:
        """Format alert as markdown."""
        return f"""
## üö® Overrun Watch Alert

**Project**: {result['project_id']}  
**Type**: {result['target']}  
**Prediction**: **{result['prediction_label']}**  
**Confidence**: {result['confidence_pct']}  
**Model Status**: {result['model_status']}  

### Recommendation:
{result['recommendation']}
"""
    
    def _format_html_alert(self, result: Dict) -> str:
        """Format alert as HTML."""
        color = 'red' if result['prediction'] == 1 else 'green'
        return f"""
<div style="border: 2px solid {color}; padding: 15px; border-radius: 5px;">
    <h3>üö® Overrun Watch Alert</h3>
    <p><strong>Project:</strong> {result['project_id']}</p>
    <p><strong>Type:</strong> {result['target']}</p>
    <p><strong>Prediction:</strong> <span style="color: {color};">{result['prediction_label']}</span></p>
    <p><strong>Confidence:</strong> {result['confidence_pct']}</p>
    <p><strong>Model Status:</strong> {result['model_status']}</p>
    <hr>
    <p><strong>Recommendation:</strong> {result['recommendation']}</p>
</div>
"""


# ============================================================================
# EXAMPLE USAGE
# ============================================================================

if __name__ == "__main__":
    print("\n" + "=" * 80)
    print("OVERRUN WATCH API - EXAMPLE USAGE")
    print("=" * 80 + "\n")
    
    # Initialize predictor
    predictor = OverrunPredictor(model_dir='saved_models')
    
    print("\n" + "-" * 80)
    print("EXAMPLE 1: Single Project Prediction (TIME)")
    print("-" * 80)
    
    # Load test data to get example features
    try:
        with open('../models/prepared_data/modeling_datasets.pkl', 'rb') as f:
            datasets = pickle.load(f)
        
        # Get one test sample for TIME prediction
        X_test_time = datasets['X_test_time']
        sample_time = X_test_time.iloc[0:1]
        
        # Make prediction
        result_time = predictor.predict_time_overrun(
            sample_time, 
            project_id="Project_Alpha"
        )
        
        print("\nüìä TIME Overrun Prediction:")
        print(f"  Project: {result_time['project_id']}")
        print(f"  Prediction: {result_time['prediction_label']}")
        print(f"  Confidence: {result_time['confidence_pct']}")
        print(f"  Status: {result_time['model_status']}")
        print(f"  Recommendation: {result_time['recommendation']}")
        
        # Generate alert
        print("\nüìß Generated Alert (Markdown):")
        alert = predictor.generate_alert(result_time, format='markdown')
        print(alert)
        
    except FileNotFoundError:
        print("‚ö†Ô∏è  Test data not found. Using dummy features for demonstration.")
        print("   Run the EDA_corr.ipynb notebook first to generate test data.")
    
    print("\n" + "-" * 80)
    print("EXAMPLE 2: Feature Importance")
    print("-" * 80)
    
    # Get top features
    print("\nüìä Top 10 Features for TIME Overrun:")
    importance_time = predictor.get_feature_importance('time', top_n=10)
    print(importance_time.to_string(index=False))
    
    print("\nüí∞ Top 10 Features for COST Overrun:")
    importance_cost = predictor.get_feature_importance('cost', top_n=10)
    print(importance_cost.to_string(index=False))
    
    print("\n" + "=" * 80)
    print("‚úÖ API DEMONSTRATION COMPLETE")
    print("=" * 80)
    print("\nNext Steps:")
    print("  1. Integrate with your project management system")
    print("  2. Set up automated daily predictions")
    print("  3. Configure email/dashboard alerts for top-k projects")
    print("  4. Monitor model performance and retrain periodically")
    print("=" * 80 + "\n")
